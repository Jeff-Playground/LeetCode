Consistent hashing: adding a new node wouldn't change much of the original nodes, so it's consistent
    Hashing ring range: 0 - 2^64-1 (Java long)
    Micro shards/Virtual nodes: each machine can be mapped to a lot of(for example 1000) micro shards on the ring
                                each micro shard is a node/value on the ring
        When computing the machine location of a value, just hash it to map to a node on the ring, and find the first existing node on the ring clock-wise
            e.g. Hash(value)=location, lookup location in TreeMap(RBTree) to find the corresponding machine
        When adding a new machine, each new node would get part of the data from the first existing node clock-wise on the ring
Backup vs. Replica:
    Backup: static, and is done on a regular basis at a scheduled time
            when data loss occurs, it can only recover to one of the backup time snapshot
            it's offline and not used for read requests
    Replica: real-time, is done while doing write requests to multiple copies
             when data loss occurs, it can use one of the duplicate nodes for recovering immediately
             it's online and can be used for read requests
             it's more expensive than backup
SQL replica: (MySQL)
    Master-slave:
        Master handles write and read requests while slave only read requests
        Write ahead log: All requests are persisted in log, and master will notify slave to read logs when new requests happens
                         Data on slaves are delayed
        When master is down, one of the slaves will be promoted to master and start handling read and write requests
NoSQL replica: (Cassandra)
    Data are stored on 3 clock-wise virtual nodes on the hashing ring belong to 3 different machines
SQL vs. NoSQL replica:
    SQL replica is usually shipped by master-slave setup, it can be manually configured to use consistent hashing
    NoSQL replica is automatically by consistent hashing, so it requires less effort for replica and sharding
Design Tiny URL:
    Scenario:
        Generate a short URL based on a long URL
        Can retrieve and redirect(http status code 301) user to the mapped long URL given a short URL
        Provide customized url for special urls(can be done by adding special short_url)
        QPS:
            DAU
            Request per user per day
        Data size
    Service:
        GET     /<short_url>
                return a HTTP redirect response
        POST    /data/shorten/
                Data = {url: <long_url>}
                return <short_url>
        short_url generation algorithm:
            Hash function: (MD5)
                quick
                hard to avoid collision - not preferred
            Random generation then check against DB short_url key:
                simple
                generation becomes slower as time goes by(regenerations more often due to collision)
            Base62 on DB sequential ID: 6 bits short_url has 62^6=57B of capacity
                depends on a global sequential ID
    Storage:
        SQL vs. NoSQL
            NoSQL doesn't support transaction
            SQL has better query support
            SQL has sequential ID
            NoSQL has better performance
            NoSQL is usually shipped with scalability
        short_url random generation:
            SQL:
                use ONE table and create index for both short_url and long_url
            NoSQL:
                use TWO tables and create index for short_url and long_url in them
        short_url generation based on Base62 on DB sequential ID:
            SQL:
                short_url can be computed by the sequential ID
    Scale:
        How to reduce the response time:
            Cache: (when too many reads need to be handled)
                Cache aside
                Cache through
            Optimize basing on user geographic location:
                Direct users to servers in different locations based on DNS parsing
                The system can have single database with location-distributed cache(cache aside)
                For distributed system, data sharding can be done based on DNS location, e.g. China data stored in China DB
            Add more DB nodes: (when too many writes need to be handled)
                Vertical sharding: not applicable here
                Horizontal sharding:
                    Sharding key:
                        for locating the DB node
                        hash long_url to sharding key and add it to short_url, for example: Hash(long_url) % 62, so new short_url becomes sharding_key+short_url
                        When doing long to short, put the entry on the node corresponding to the sharding key
                        When doing short to long, use sharding key to locate the node and query for the entry
        Does short_url needs an expiration date? No, it's bad for user experience and storage is cheap
        Can there be duplicates? Yes, one long_url can be mapped to multiple short_url, but short_url needs to be unique
        Global sequential ID in a distributed system:
            1. Use one DB node specifically for generating sequential IDs, note it might need replicas to avoid single point of failure issue
            2. Use Zookeeper
What happens when opening a URL in browser:
1. Request goes to closest DNS server, where it has mapping info on the ip for the given domain name
2. Browser send http/https request to the looked up ip
3. Destination webserver receive the request and give it to the HTTP server listening on port 80
4. The HTTP server sends the request to the web application
5. Web application handles the request and return a response
6. Browser get the response